<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>强化学习的数学原理 on Jerry Blog</title>
    <link>https://jerryblog.top/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 强化学习的数学原理 on Jerry Blog</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 30 Nov 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://jerryblog.top/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>强化学习的数学原理-第四章</title>
      <link>https://jerryblog.top/AA/RL_04/</link>
      <pubDate>Sun, 30 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://jerryblog.top/AA/RL_04/</guid>
      <description>&lt;h3 id=&#34;值迭代算法value-iteration&#34;&gt;值迭代算法（value iteration）&lt;/h3&gt;&#xA;&lt;p&gt;由上一章可知&#xA;$$&#xA;v_{k+1} =  \max_{\pi \in \Pi} (r_{\pi} + \gamma P_\pi v_k)&#xA;$$&#xA;当 $k \to \infty$ 时，$v_k$ 以指数速度收敛到$v^*$。&lt;/p&gt;&#xA;&lt;p&gt;值迭代算法包括两个步骤：&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;mark&gt;策略更新&lt;/mark&gt;：根据上一步计算出的&lt;strong&gt;估计值向量&lt;/strong&gt;，求解出最好的策略。&#xA;$$&#xA;\pi ^ * = \arg\max_{\pi \in \Pi} (r_\pi + \gamma P_\pi v^* )&#xA;$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>强化学习的数学原理-第三章</title>
      <link>https://jerryblog.top/AA/RL_03/</link>
      <pubDate>Mon, 17 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://jerryblog.top/AA/RL_03/</guid>
      <description>&lt;h3 id=&#34;optimal-policies&#34;&gt;optimal policies&lt;/h3&gt;&#xA;&lt;p&gt;从直觉上来看，不同的policy会导致不同的return，而强化学习基本的思想就是找到一个最优策略&lt;code&gt;optimal policies&lt;/code&gt;使得奖励最大。&lt;/p&gt;</description>
    </item>
    <item>
      <title>强化学习的数学原理-第二章</title>
      <link>https://jerryblog.top/AA/RL_02/</link>
      <pubDate>Mon, 03 Nov 2025 00:00:00 +0000</pubDate>
      <guid>https://jerryblog.top/AA/RL_02/</guid>
      <description>&lt;h2 id=&#34;强化学习的数学原理第二章&#34;&gt;强化学习的数学原理&amp;ndash;第二章&lt;/h2&gt;&#xA;&lt;p&gt;上一章提到，return十分重要，因为可以用其评估某条线路的好坏。简单来说，return可以通过定义进行计算，即沿着路线依次将reward相加；另外，回报也可以通过&lt;strong&gt;自举法&lt;/strong&gt;(bootstrapping)求出。&lt;/p&gt;</description>
    </item>
    <item>
      <title>强化学习的数学原理-第一章</title>
      <link>https://jerryblog.top/AA/RL_01/</link>
      <pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate>
      <guid>https://jerryblog.top/AA/RL_01/</guid>
      <description>&lt;h2 id=&#34;第一章--基本概念&#34;&gt;第一章 · 基本概念&lt;/h2&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;code&gt;状态 state&lt;/code&gt;：在环境中所处的状态，包括位置、速度、加速度等。&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;code&gt;状态空间 state space&lt;/code&gt;：状态空间，表示所有状态的集合&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;&lt;code&gt;动作 Action&lt;/code&gt;：动作，对于一个state有上下左右和静止五种状态。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
